hierarchical_config = {
    'attention_dim': 6,
    'attention_heads': 3,
    'postprocessing_hiddens': [128],
    'postprocessing_activation': 'relu'
}
